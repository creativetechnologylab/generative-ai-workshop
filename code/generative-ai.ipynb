{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef47f24",
   "metadata": {},
   "source": [
    "# Generative AI with Python (with some Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e271a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176bcbee",
   "metadata": {},
   "source": [
    "## Text to Image with StableDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ff20b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe54dbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]The config attributes {'predict_epsilon': True} were passed to DPMSolverMultistepScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m MODEL_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOFA-Sys/small-stable-diffusion-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m img \u001b[38;5;241m=\u001b[39m pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtasty apples\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 4K\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/gen-ai/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/gen-ai/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:1022\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;66;03m# load sub model\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m     sub_model_dtype \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1018\u001b[0m         torch_dtype\u001b[38;5;241m.\u001b[39mget(name, torch_dtype\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(torch_dtype, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m torch_dtype\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[0;32m-> 1022\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_sub_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimportable_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimportable_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipelines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_pipeline_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_pipeline_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msub_model_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43msess_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msess_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_variants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdduf_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdduf_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` subfolder of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m     )\n\u001b[1;32m   1051\u001b[0m init_kwargs[name] \u001b[38;5;241m=\u001b[39m loaded_sub_model  \u001b[38;5;66;03m# UNet(...), # DiffusionSchedule(...)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/gen-ai/lib/python3.10/site-packages/diffusers/pipelines/pipeline_loading_utils.py:830\u001b[0m, in \u001b[0;36mload_sub_model\u001b[0;34m(library_name, class_name, importable_classes, pipelines, is_pipeline_module, pipeline_class, torch_dtype, provider, sess_options, device_map, max_memory, offload_folder, offload_state_dict, model_variants, name, from_flax, variant, low_cpu_mem_usage, cached_folder, use_safetensors, dduf_entries, provider_options, quantization_config)\u001b[0m\n\u001b[1;32m    828\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m load_method(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloading_kwargs)\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cached_folder, name)):\n\u001b[0;32m--> 830\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloading_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;66;03m# else load from the root directory\u001b[39;00m\n\u001b[1;32m    833\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m load_method(cached_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloading_kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/gen-ai/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/gen-ai/lib/python3.10/site-packages/diffusers/schedulers/scheduling_utils.py:158\u001b[0m, in \u001b[0;36mSchedulerMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, return_unused_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03mInstantiate a scheduler from a pre-defined JSON configuration file in a local directory or Hub repository.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    151\u001b[0m config, kwargs, commit_hash \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mload_config(\n\u001b[1;32m    152\u001b[0m     pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39mpretrained_model_name_or_path,\n\u001b[1;32m    153\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/gen-ai/lib/python3.10/site-packages/diffusers/configuration_utils.py:263\u001b[0m, in \u001b[0;36mConfigMixin.from_config\u001b[0;34m(cls, config, return_unused_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         init_dict[deprecated_kwarg] \u001b[38;5;241m=\u001b[39m unused_kwargs\u001b[38;5;241m.\u001b[39mpop(deprecated_kwarg)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Return model and optionally state and/or unused_kwargs\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# make sure to also save config parameters that might be used for compatible classes\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# update _class_name\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_class_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m hidden_dict:\n",
      "File \u001b[0;32m~/miniforge3/envs/gen-ai/lib/python3.10/site-packages/diffusers/configuration_utils.py:693\u001b[0m, in \u001b[0;36mregister_to_config.<locals>.inner_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_init_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs}\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_to_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs)\n\u001b[0;32m--> 693\u001b[0m \u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/gen-ai/lib/python3.10/site-packages/diffusers/schedulers/scheduling_dpmsolver_multistep.py:298\u001b[0m, in \u001b[0;36mDPMSolverMultistepScheduler.__init__\u001b[0;34m(self, num_train_timesteps, beta_start, beta_end, beta_schedule, trained_betas, solver_order, prediction_type, thresholding, dynamic_thresholding_ratio, sample_max_value, algorithm_type, solver_type, lower_order_final, euler_at_final, use_karras_sigmas, use_exponential_sigmas, use_beta_sigmas, use_lu_lambdas, use_flow_sigmas, flow_shift, final_sigmas_type, lambda_min_clipped, variance_type, timestep_spacing, steps_offset, rescale_betas_zero_snr)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_inference_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    297\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, num_train_timesteps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, num_train_timesteps, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m solver_order\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlower_order_nums \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"OFA-Sys/small-stable-diffusion-v0\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "img = pipe(\"tasty apples\"+ \" 4K\").images[0]\n",
    "img.save(\"output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91e249b",
   "metadata": {},
   "source": [
    "## Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754f3b4",
   "metadata": {},
   "source": [
    "### What are Large Language Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a17cd",
   "metadata": {},
   "source": [
    "Large Language Models \n",
    "\n",
    "\"auto-correct on steroids\"\n",
    "\n",
    "[A short introduction to LLMs](https://www.youtube.com/watch?v=LPZh9BOjkQs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01640ab8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Ollama & Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf3a8f",
   "metadata": {},
   "source": [
    "**Ollama** is a tool that allows us to run LLMs locally. It can be downloaded and used entirely for _free_.\n",
    "\n",
    "But what does it mean to run something _locally_? That means you're running it _solely_ on your own machine, rather than sending information back and forth with an online service.\n",
    "\n",
    "This has some key advantages:\n",
    "- cost\n",
    "- privacy\n",
    "- doesn't depend on stable/fast internet access\n",
    "- peformance isn't affected by how many other people are using the same online services at a given time\n",
    "\n",
    "To test our Ollama installation, we can see the output from inputting `ollama` in the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b72f1",
   "metadata": {},
   "source": [
    "It's also possible to do this within Python by using the `subprocess` library. So that's one option..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0af523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from command line:\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage:\n",
      "  ollama [flags]\n",
      "  ollama [command]\n",
      "\n",
      "Available Commands:\n",
      "  serve       Start ollama\n",
      "  create      Create a model from a Modelfile\n",
      "  show        Show information for a model\n",
      "  run         Run a model\n",
      "  stop        Stop a running model\n",
      "  pull        Pull a model from a registry\n",
      "  push        Push a model to a registry\n",
      "  list        List models\n",
      "  ps          List running models\n",
      "  cp          Copy a model\n",
      "  rm          Remove a model\n",
      "  help        Help about any command\n",
      "\n",
      "Flags:\n",
      "  -h, --help      help for ollama\n",
      "  -v, --version   Show version information\n",
      "\n",
      "Use \"ollama [command] --help\" for more information about a command.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run the `echo` command and capture output\n",
    "result = subprocess.run([\"ollama\"], text=True)\n",
    "\n",
    "print(\"Output from command line:\")\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f3473",
   "metadata": {},
   "source": [
    "This gives us a list of commands that we can use with Ollama. For our purposes, we're mainly concerned with being able to pull models, list what models are on our system, and remove the ones we no longer want to use. In a fresh installtion, Ollama comes with zero models, but the script has _pulled_ a few already to make things easier. We can use the ollama-python library to list these models. Of course, the command line works too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831d1ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(model='glm4:latest', modified_at=datetime.datetime(2025, 7, 16, 15, 7, 31, 79556, tzinfo=TzInfo(+01:00)), digest='5b699761eca535dc55047ad9d2dbf54e3b8697709419ef78a70503ed4bfbcf44', size=5455326235, details=ModelDetails(parent_model='', format='gguf', family='chatglm', families=['chatglm'], parameter_size='9.4B', quantization_level='Q4_0')),\n",
       " Model(model='deepseek-r1:7b', modified_at=datetime.datetime(2025, 7, 16, 14, 52, 22, 708686, tzinfo=TzInfo(+01:00)), digest='755ced02ce7befdb13b7ca74e1e4d08cddba4986afdb63a480f2c93d3140383f', size=4683075440, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='7.6B', quantization_level='Q4_K_M')),\n",
       " Model(model='dolphin-phi:latest', modified_at=datetime.datetime(2025, 7, 16, 11, 21, 6, 952843, tzinfo=TzInfo(+01:00)), digest='c5761fc772409945787240af89a5cce01dd39dc52f1b7b80d080a1163e8dbe10', size=1602473850, details=ModelDetails(parent_model='', format='gguf', family='phi2', families=['phi2'], parameter_size='3B', quantization_level='Q4_0')),\n",
       " Model(model='cogito:32b', modified_at=datetime.datetime(2025, 5, 5, 5, 19, 4, 7661, tzinfo=TzInfo(+01:00)), digest='0b4aab772f57ddf3d9e46235ef6f142e42598fd121dc15588c7f06de786c510e', size=19848516836, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='32.8B', quantization_level='Q4_K_M')),\n",
       " Model(model='JollyLlama/GLM-4-32B-0414-Q4_K_M:latest', modified_at=datetime.datetime(2025, 5, 2, 0, 25, 50, 275690, tzinfo=TzInfo(+01:00)), digest='d61b44b6a5d3234dd25dd525546e895b9431a3b2b798666d154844fdeb3e0129', size=19680022745, details=ModelDetails(parent_model='', format='gguf', family='glm4', families=['glm4'], parameter_size='32.6B', quantization_level='Q4_K_M')),\n",
       " Model(model='EntropyYue/longwriter-glm4:9b', modified_at=datetime.datetime(2025, 5, 1, 23, 44, 15, 434703, tzinfo=TzInfo(+01:00)), digest='6fb2068f2b4362b0b5e6d54c907007e854d5a7347bd8ebe3324defbb027b8fe8', size=5455324148, details=ModelDetails(parent_model='', format='gguf', family='chatglm', families=['chatglm'], parameter_size='9.4B', quantization_level='Q4_0'))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.list().models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676a80e",
   "metadata": {},
   "source": [
    "That is quite a bit of information, so we can go through this _return value_ to extract just the information that's more human-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "af2f2ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm4:latest\n",
      "deepseek-r1:7b\n",
      "dolphin-phi:latest\n",
      "cogito:32b\n",
      "JollyLlama/GLM-4-32B-0414-Q4_K_M:latest\n",
      "EntropyYue/longwriter-glm4:9b\n"
     ]
    }
   ],
   "source": [
    "for model in ollama.list().models:\n",
    "    print(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202453ea",
   "metadata": {},
   "source": [
    "Now we have a plain list of the models on the system - this shows us what was downloaded (or _pulled_) by running the installation script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2dbfc",
   "metadata": {},
   "source": [
    "To start with, I'm going to create a _variable_ for storing the name of the model I wish to use. This is going to be a _parameter_ that we give repeatedly to the ollama python library, so it makes sense to write it down once and avoid repeating ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd9a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dophin-phi is 2.7b\n",
    "DOLPHIN_PHI = \"dolphin-phi\"\n",
    "# this particular deepseek model is 7b\n",
    "DEEPSEEK = \"deepseek-r1:7b\"\n",
    "# glm4 9b version\n",
    "GLM4 = \"glm4:latest\"\n",
    "# moondream\n",
    "MOONDREAM = \"moondream\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a1111",
   "metadata": {},
   "source": [
    "A convention when programming in Python is to write constants -- variables that are set once and never changes -- in all-caps. This doesn't affect how your code runs, but it can be nice for making things more ordered. I feel it tells me this bit of information is \"important\" in some way, while using less mental effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97bf6360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris. \n",
      "\n",
      "Paris is located on the Seine River in the north-central part of France and is known for its rich history, culture, fashion, art, and cuisine. It is also home to several famous landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and many others. Paris serves as both the capital city of France and the European Union's headquarters.\n",
      "\n",
      "Please note that this information was last updated in October 2021 by AI model GPT-3. For current updates or more specific details, it is recommended to consult a reliable source like an official government website or a trusted encyclopedic database.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "response = chat(model=DOLPHIN_PHI, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'What is the capital of France?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87600bd5",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89a0c623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue due to the phenomenon known as Rayleigh scattering. When sunlight enters the Earth's atmosphere, it interacts with molecules in the air such as nitrogen and oxygen. These molecules are much smaller than the wavelength of visible light (around 400-700 nm), which is why they can scatter the shorter-wavelength colors more effectively, like blue.\n",
      "\n",
      "However, the sky isn't really blue at all. The light from the sun gets scattered in all directions by these molecules in the atmosphere and our eyes perceive this as a \"blue\" color. When the sunlight passes through Earth's atmosphere, it is broken up into its different colors due to Rayleigh scattering. Blue light has a shorter wavelength, so it is scattered more than the other colors of visible light, which are longer in wavelength. This makes the sky appear blue from our perspective on the ground.\n",
      "\n",
      "The deeper you look into the sky, the further away the sun is and the longer the path sunlight travels, causing more scattering of shorter wavelengths of light to be absorbed by the atmosphere. At sunset or sunrise when the sun is low on the horizon, the light has to pass through a larger portion of Earth's atmosphere, which scatters even more blue light and causes the sky to turn various shades of red and orange."
     ]
    }
   ],
   "source": [
    "stream = chat(\n",
    "    model=DOLPHIN_PHI,\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8907377",
   "metadata": {},
   "source": [
    "### Vision Language Models (VLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564dff12",
   "metadata": {},
   "source": [
    "Vision Language Models can be used to describe images. Let's try this out with this clown image.\n",
    "\n",
    "![](../pictures/clown.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de818b20",
   "metadata": {},
   "source": [
    "First, we need to load the image. To do this, we need ot make use of the `base64` library as it allows us to convert the image into a format that a VLM can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da96d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# load an image as base64\n",
    "with open(\"../pictures/clown.jpg\", \"rb\") as image_file:\n",
    "    data = base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd6efa",
   "metadata": {},
   "source": [
    "Now that the image has been loaded, we can send it to the VLM `moondream`, and ask it to tell us what the image contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c33443f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The image features a clown with a red wig and polka dot suit, standing against a white background. The clown is making a funny face for the camera while holding up his hands as if to mimic a speech bubble or an exaggerated \"O\" shape.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=MOONDREAM,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's in this image?\",\n",
    "            \"images\": [data], # pass the image in the images field\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35931289",
   "metadata": {},
   "source": [
    "We can also ask moondream to explain certain details in the image to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81fea084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The clown's nose is red.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=MOONDREAM,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What colour is his nose?\",\n",
    "            \"images\": [data], # pass the image in the images field\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae59208",
   "metadata": {},
   "source": [
    "We can see a list of vision models that work with Ollama here: https://ollama.com/search?c=vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3468c26c",
   "metadata": {},
   "source": [
    "### Small Language Models\n",
    "\n",
    "Language Models come in very small sizes too. Some examples include `smollm` and `tinyllama`. While these models are more prone to hallucination, and have more limited \"intelligence,\" they can run quite fast even on less powerful hardware such as Raspberry Pis and computers with older GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e60b6b",
   "metadata": {},
   "source": [
    "### Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91aee88",
   "metadata": {},
   "source": [
    "![](../pictures/how-to-cook-your-dragon.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "07d135c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"Dragon Meat Cookbook\" by John Doe - This comprehensive cookbook offers various recipes using dragon meat, along with detailed cooking techniques and nutritional information.\n",
      "2. \"Cooking with Dragon Meat: A Guide for the Adventurous Chef\" by Jane Smith - It provides a wide range of creative and delicious recipes that showcase the versatility of dragon meat.\n",
      "3. \"Dragon Cuisine: The Ultimate Guide to Cooking with Dragon Meat\" by Michael Johnson - This book not only offers tasty recipes but also delves into the cultural and historical aspects of cooking with dragon meat.\n",
      "4. \"The Dragon's Feast Cookbook\" by Sarah Lee - A collection of authentic recipes from different regions that feature dragon meat, making it a great resource for global culinary exploration.\n",
      "5. \"Dragon Meat: Cooking with a Mythical Ingredient\" by David Brown - This book focuses on the unique flavors and textures of dragon meat, providing guidance on how to cook it in a variety of dishes.\n",
      "6. \"The Dragon Chef's Cookbook\" by Emily Chen - A collection of easy-to-follow recipes that cater to different dietary preferences, making it an accessible resource for home cooks.\n",
      "7. \"Dragon Meat: The Culinary Adventure\" by William Taylor - This cookbook is packed with creative and innovative dragon meat recipes, along with tips on how to source and prepare the meat.\n",
      "8. \"The Dragon Cookbook\" by Jennifer Lee - A unique and entertaining guide that features a collection of funny anecdotes, trivia, and stories related to dragon meat.\n",
      "9. \"Dragon Meats: From Dragon Roasting to Dragon Scallop\" by Robert Brown - This in-depth guide provides expert insights into the history, preparation, and culinary uses of dragon meat.\n",
      "10. \"The Dragon Chef's Guide to Cooking with Fire\" by Samantha Smith - A resource for those looking to cook with a variety of meats, including dragon, over an open fire.\n"
     ]
    }
   ],
   "source": [
    "response = chat(model=DOLPHIN_PHI, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'What are some good cookbooks on how to use dragon meat?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34632e5",
   "metadata": {},
   "source": [
    "### Thinking Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda3a7b",
   "metadata": {},
   "source": [
    "Explanation of thinking/reasoning models goes here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f879955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out what the capital of France is. Hmm, I think I know a few countries and their capitals, but I'm not 100% sure about France. Let me try to remember or visualize it.\n",
      "\n",
      "I recall that Paris is a major city in France. It's got the Eiffel Tower there, which is a famous landmark. I've heard people say \"LeEifel\" and they sometimes refer to it as the capital because so many important things are happening in Paris. There's also the Louvre Museum, which is probably in the city center.\n",
      "\n",
      "Wait, but could there be another city that's more prominent? I think of other European capitals like Berlin, Madrid, London—those are definitely capitals. But for France, it's always been Paris. I don't think there's any doubt about it anymore because I've heard from so many people who were born there or have strong connections to it.\n",
      "\n",
      "So putting this together, the capital must be Paris.\n",
      "</think>\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = chat(model=DEEPSEEK, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'What is the capital of France?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893821b",
   "metadata": {},
   "source": [
    "#### The Strawberry Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4788485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how many times the letter \"R\" appears in the word \"strawberry.\" Hmm, let's break this down step by step. First off, I know that each word is made up of letters, and sometimes vowels like \"A,\" \"E,\" \"I,\" \"O,\" \"U\" can also be considered depending on the context, but here we're specifically looking for the letter \"R.\"\n",
      "\n",
      "Alright, so the word in question is \"strawberry.\" Let me spell it out to visualize each letter clearly. S-T-R-A-W-B-E-R-R-Y. Wait, that might not be exactly right because sometimes people can miscount letters when spelling quickly.\n",
      "\n",
      "Let me write it down slowly: s-t-r-a-w-b-e-r-r-y. Hmm, I think I see the \"r\" in there a couple of times. Let's go through each letter one by one to make sure I don't miss any or count extra ones.\n",
      "\n",
      "Starting with S (1), then T (2), R (3) – that's the first R. Next is A (4). Then W (5). B (6), E (7). Now, here comes R again at position 8 (8), and another one after that at position 9 (9). Finally, Y at position 10.\n",
      "\n",
      "Wait a second, so I counted three Rs: positions 3, 8, and 9. But hold on, sometimes people might get confused because the \"berry\" part has two Es but also ends with \"ry,\" which adds another R. So in total, is it three?\n",
      "\n",
      "Alternatively, maybe I should write out each letter and mark the R's as I go:\n",
      "\n",
      "1: S\n",
      "2: T\n",
      "3: R (1st R)\n",
      "4: A\n",
      "5: W\n",
      "6: B\n",
      "7: E\n",
      "8: R (2nd R)\n",
      "9: R (3rd R)\n",
      "10: Y\n",
      "\n",
      "Yes, that seems to confirm it. So there are three Rs in \"strawberry.\" But wait, I thought maybe sometimes people might think only two because of the double R at the end, but no, actually, \"berry\" is spelled with one R before the Y, making it two more after the initial R.\n",
      "\n",
      "Let me just check a dictionary or a reliable source to confirm. Well, without looking it up, based on how I spelled it out letter by letter, there are three Rs in \"strawberry.\" So my answer should be that the letter R appears three times in the word.\n",
      "</think>\n",
      "\n",
      "The letter R appears 3 times in the word \"strawberry.\"\n",
      "\n",
      "**Step-by-Step Explanation:**\n",
      "\n",
      "1. **Write Out the Word:** Start by spelling out the word \"strawberry\" to visualize each letter clearly.\n",
      "   - s-t-r-a-w-b-e-r-r-y\n",
      "\n",
      "2. **Identify Each Letter:** Assign a number to each letter as you spell them one after another.\n",
      "   - 1: S\n",
      "   - 2: T\n",
      "   - 3: R (First occurrence of R)\n",
      "   - 4: A\n",
      "   - 5: W\n",
      "   - 6: B\n",
      "   - 7: E\n",
      "   - 8: R (Second occurrence of R)\n",
      "   - 9: R (Third occurrence of R)\n",
      "   - 10: Y\n",
      "\n",
      "3. **Count the Rs:** Mark each time you encounter the letter \"R.\"\n",
      "   - First R at position 3.\n",
      "   - Second R at position 8.\n",
      "   - Third R at position 9.\n",
      "\n",
      "4. **Conclusion:** After counting all instances, it's clear that the letter R appears three times in \"strawberry.\"\n",
      "\n",
      "**Answer:** The letter R appears 3 times in the word strawberry."
     ]
    }
   ],
   "source": [
    "stream = chat(\n",
    "    model=DEEPSEEK,\n",
    "    messages=[{'role': 'user', 'content': 'How many times does the letter R appear in the word strawberry?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b51356",
   "metadata": {},
   "source": [
    "### Finding the \"Best\" Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c442ec9",
   "metadata": {},
   "source": [
    "trade-offs with sensible output and size/speed  \n",
    "trial and error experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6a628",
   "metadata": {},
   "source": [
    "We can create a quick comparison test by asking various models to generate text based on the same prompt, and see which output we like the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc8c92d",
   "metadata": {},
   "source": [
    "Firstly, we can take all the models that are on the system right now, and place them in a Python list. This will make things easier in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df182f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [DOLPHIN_PHI, DEEPSEEK, GLM4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a6014",
   "metadata": {},
   "source": [
    "Now, we can create a _function_ for sending the same prompt to different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b774de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limerick_creator(model: str):\n",
    "    response = chat(model=model, messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Write a limerick about the nature of time.',\n",
    "    },\n",
    "    ])\n",
    "    \n",
    "    print(\"Model:\", model)\n",
    "    print(response['message']['content'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf4f7a",
   "metadata": {},
   "source": [
    "Now we can _call_ this function with our different models, and see how the output varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28689e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: dolphin-phi\n",
      "There once was a concept called time,\n",
      "Its ticking clock could not be chime,\n",
      "It flowed like a river swift,\n",
      "Past and future with a shift,\n",
      "Leaving moments in its wake so bright.\n",
      "\n",
      "\n",
      "Model: deepseek-r1:7b\n",
      "<think>\n",
      "Alright, so I need to write a limerick about the nature of time. Hmm, okay. First off, what is a limerick? From what I remember, it's a five-line poem with an AABBA rhyme scheme. It usually has a playful or rhythmic feel to it and often tells a short story or conveys a light-hearted message.\n",
      "\n",
      "Now, the topic here is the nature of time. Time can be tricky because it's something we all experience every day but isn't tangible—it's more of an abstract concept. I guess I could approach this in different ways: maybe talking about how time moves us forward without pause, or perhaps the idea that time doesn't care who waits for it.\n",
      "\n",
      "I should think about imagery related to time—maybe clocks, the ticking sound, seasons passing, aging, or moments rushing by too quickly. Using some rhyme and rhythm here will be key. Let me try to come up with a structure: first line introduces the subject of time; second and third lines delve into its nature or behavior; fourth line might offer a perspective on how it's perceived; fifth line is usually a punchline or a conclusion.\n",
      "\n",
      "Let me see... Maybe start with something like \"There once was a time, oh what fun,\" but wait, that doesn't fit the limerick structure. Alternatively, I could try to think of a metaphor involving movement since time often feels like it's moving forward.\n",
      "\n",
      "Wait, another thought: perhaps using the image of a river running or a child skipping stones over water because time glides by quickly. That could tie into the idea of time passing without stopping and how we don't notice it when it does go by.\n",
      "\n",
      "Putting that together:\n",
      "\n",
      "\"There once was a river so clear,\"  \n",
      "\"Where time's waves seemed to skip.\"  \n",
      "But then I need to fit in something about its nature, maybe how time doesn't wait for anyone. Let me try another angle: comparing time to something that's always moving without rest.\n",
      "\n",
      "Maybe something like \"Time has no bounds, it goes on and on,\" but make it rhyme. Hmm, perhaps using \"snaps through the fingers of man\" from Alice in Wonderland as a metaphor? Not sure if that's the best fit here.\n",
      "\n",
      "Wait, let me try to think of some lines:\n",
      "\n",
      "\"The clock ticks softly, soft as a bell,\"  \n",
      "\"But time is faster, quicker than we'll see.\"  \n",
      "But then I need more lines. Maybe add something about how it doesn't stop for anyone and how we often resist its flow.\n",
      "\n",
      "Wait, maybe the first line could be \"Time is a thief\" or \"Time moves swiftly.\" Let me try that:\n",
      "\n",
      "\"There's a thief in the night, quick as can be,\"  \n",
      "\"Who steals away moments when we're not asleep.\"  \n",
      "That has rhyme and rhythm. Now for the next lines: perhaps how time doesn't wait for anyone.\n",
      "\n",
      "\"But he takes what he wants, with little pause,\"  \n",
      "\"He leaves our days before our years can save.\"  \n",
      "Now I'm at four lines. The fifth line should wrap it up or add a twist. Maybe something like \"So hurry up and be ready\" to catch up? But that's more of a command than a statement about time.\n",
      "\n",
      "Alternatively, maybe the last couplet could express a perspective on life in relation to time:\n",
      "\n",
      "\"So watch the moments as they pass by,\"  \n",
      "\"Time's a trickster, always changes my way.\"  \n",
      "Hmm, not sure if that's strong enough. Let me try again with different wording for the last two lines.\n",
      "\n",
      "Perhaps something like \"Time moves on without pause, It takes what it will, and leaves its trace.\"\n",
      "\n",
      "Wait, putting it all together:\n",
      "\n",
      "\"There was once a thief in the night,\"  \n",
      "\"Who steals away moments when we rest.\"  \n",
      "\"He takes what he wants, with little delay,\"  \n",
      "\"And leaves our days before our years grow old.\"  \n",
      "\"So watch the moments as they pass by,\"  \n",
      "\"Time's a trickster, always changes my way.\"\n",
      "\n",
      "Hmm, that works. It follows the AABBA rhyme scheme and touches on time moving quickly without pause, taking what it wants, leaving in its wake, making us realize how fast our days fly.\n",
      "\n",
      "I think this captures the essence of the limerick structure while conveying something about time's nature—its swift movement and impact on our lives.\n",
      "</think>\n",
      "\n",
      "There once was a thief in the night,  \n",
      "Who steals away moments when we rest.  \n",
      "He takes what he wants, with little delay,  \n",
      "And leaves our days before our years grow old.  \n",
      "\n",
      "So watch the moments as they pass by,  \n",
      "Time's a trickster, always changes my way.\n",
      "\n",
      "\n",
      "Model: glm4:latest\n",
      "There once was time so swift and fleet,  \n",
      "With moments that could seem complete;  \n",
      "But then it's past, as if a dream,  \n",
      "And each second dances in a beam —  \n",
      "Nature’s clock keeps on a tick.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    limerick_creator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053a57f",
   "metadata": {},
   "source": [
    "## Other ML Tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
