{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef47f24",
   "metadata": {},
   "source": [
    "# Generative AI with Python (with some Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e271a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176bcbee",
   "metadata": {},
   "source": [
    "## Text to Image with StableDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff20b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    DiffusionPipeline,\n",
    "    AutoPipelineForText2Image,\n",
    "    DEISMultistepScheduler,\n",
    ")\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe54dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"OFA-Sys/small-stable-diffusion-v0\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "img = pipe(\"alien invasion, soft lighting, realistic, 4k\").images[0]\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce1637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"segmind/tiny-sd\", torch_dtype=torch.float16\n",
    ")\n",
    "prompt = \"stonehenge, sharp details, realistic, 4K\"\n",
    "pipeline = pipeline.to(\"cuda\")\n",
    "image = pipeline(prompt).images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"nitrosocke/Ghibli-Diffusion\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"ghibli style green ogre\"\n",
    "image = pipe(prompt).images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ce22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"dreamlike-art/dreamlike-diffusion-1.0\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\"\n",
    "image = pipe(prompt).images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285507f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"lykon/dreamshaper-8\", torch_dtype=torch.float16, variant=\"fp16\"\n",
    ")\n",
    "pipe.scheduler = DEISMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"portrait photo of blue alien, light bokeh, intricate, steel metal, elegant, sharp focus, soft lighting, vibrant colors\"\n",
    "\n",
    "# generator = torch.manual_seed(33)\n",
    "image = pipe(prompt, generator=generator, num_inference_steps=25).images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91e249b",
   "metadata": {},
   "source": [
    "## Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754f3b4",
   "metadata": {},
   "source": [
    "### What are Large Language Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a17cd",
   "metadata": {},
   "source": [
    "Large Language Models \n",
    "\n",
    "\"auto-correct on steroids\"\n",
    "\n",
    "[A short introduction to LLMs](https://www.youtube.com/watch?v=LPZh9BOjkQs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01640ab8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Ollama & Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf3a8f",
   "metadata": {},
   "source": [
    "**Ollama** is a tool that allows us to run LLMs locally. It can be downloaded and used entirely for _free_.\n",
    "\n",
    "But what does it mean to run something _locally_? That means you're running it _solely_ on your own machine, rather than sending information back and forth with an online service.\n",
    "\n",
    "This has some key advantages:\n",
    "- cost\n",
    "- privacy\n",
    "- doesn't depend on stable/fast internet access\n",
    "- peformance isn't affected by how many other people are using the same online services at a given time\n",
    "\n",
    "To test our Ollama installation, we can see the output from inputting `ollama` in the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b72f1",
   "metadata": {},
   "source": [
    "It's also possible to do this within Python by using the `subprocess` library. So that's one option..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run the `echo` command and capture output\n",
    "result = subprocess.run([\"ollama\"], text=True)\n",
    "\n",
    "print(\"Output from command line:\")\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f3473",
   "metadata": {},
   "source": [
    "This gives us a list of commands that we can use with Ollama. For our purposes, we're mainly concerned with being able to pull models, list what models are on our system, and remove the ones we no longer want to use. In a fresh installtion, Ollama comes with zero models, but the script has _pulled_ a few already to make things easier. We can use the ollama-python library to list these models. Of course, the command line works too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d1ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.list().models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676a80e",
   "metadata": {},
   "source": [
    "That is quite a bit of information, so we can go through this _return value_ to extract just the information that's more human-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ollama.list().models:\n",
    "    print(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202453ea",
   "metadata": {},
   "source": [
    "Now we have a plain list of the models on the system - this shows us what was downloaded (or _pulled_) by running the installation script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2dbfc",
   "metadata": {},
   "source": [
    "To start with, I'm going to create a _variable_ for storing the name of the model I wish to use. This is going to be a _parameter_ that we give repeatedly to the ollama python library, so it makes sense to write it down once and avoid repeating ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd9a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dophin-phi is 2.7b\n",
    "DOLPHIN_PHI = \"dolphin-phi\"\n",
    "# this particular deepseek model is 7b\n",
    "DEEPSEEK = \"deepseek-r1:7b\"\n",
    "# glm4 9b version\n",
    "GLM4 = \"glm4:latest\"\n",
    "# moondream\n",
    "MOONDREAM = \"moondream\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a1111",
   "metadata": {},
   "source": [
    "A convention when programming in Python is to write constants -- variables that are set once and never changes -- in all-caps. This doesn't affect how your code runs, but it can be nice for making things more ordered. I feel it tells me this bit of information is \"important\" in some way, while using less mental effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "response = chat(\n",
    "    model=DOLPHIN_PHI,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87600bd5",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = chat(\n",
    "    model=DOLPHIN_PHI,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8907377",
   "metadata": {},
   "source": [
    "### Vision Language Models (VLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564dff12",
   "metadata": {},
   "source": [
    "Vision Language Models can be used to describe images. Let's try this out with this clown image.\n",
    "\n",
    "![](../pictures/clown.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de818b20",
   "metadata": {},
   "source": [
    "First, we need to load the image. To do this, we need ot make use of the `base64` library as it allows us to convert the image into a format that a VLM can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da96d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# load an image as base64\n",
    "with open(\"../pictures/clown.jpg\", \"rb\") as image_file:\n",
    "    data = base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd6efa",
   "metadata": {},
   "source": [
    "Now that the image has been loaded, we can send it to the VLM `moondream`, and ask it to tell us what the image contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33443f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "    model=MOONDREAM,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's in this image?\",\n",
    "            \"images\": [data],  # pass the image in the images field\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35931289",
   "metadata": {},
   "source": [
    "We can also ask moondream to explain certain details in the image to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fea084",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "    model=MOONDREAM,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What colour is his nose?\",\n",
    "            \"images\": [data],  # pass the image in the images field\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae59208",
   "metadata": {},
   "source": [
    "We can see a list of vision models that work with Ollama here: https://ollama.com/search?c=vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3468c26c",
   "metadata": {},
   "source": [
    "### Small Language Models\n",
    "\n",
    "Language Models come in very small sizes too. Some examples include `smollm` and `tinyllama`. While these models are more prone to hallucination, and have more limited \"intelligence,\" they can run quite fast even on less powerful hardware such as Raspberry Pis and computers with older GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e60b6b",
   "metadata": {},
   "source": [
    "### Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91aee88",
   "metadata": {},
   "source": [
    "![](../pictures/how-to-cook-your-dragon.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d135c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\n",
    "    model=DOLPHIN_PHI,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are some good cookbooks on how to use dragon meat?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34632e5",
   "metadata": {},
   "source": [
    "### Thinking Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda3a7b",
   "metadata": {},
   "source": [
    "Explanation of thinking/reasoning models goes here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f879955",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\n",
    "    model=DEEPSEEK,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893821b",
   "metadata": {},
   "source": [
    "#### The Strawberry Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4788485",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = chat(\n",
    "    model=DEEPSEEK,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many times does the letter R appear in the word strawberry?\",\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b51356",
   "metadata": {},
   "source": [
    "### Finding the \"Best\" Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c442ec9",
   "metadata": {},
   "source": [
    "trade-offs with sensible output and size/speed  \n",
    "trial and error experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6a628",
   "metadata": {},
   "source": [
    "We can create a quick comparison test by asking various models to generate text based on the same prompt, and see which output we like the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc8c92d",
   "metadata": {},
   "source": [
    "Firstly, we can take all the models that are on the system right now, and place them in a Python list. This will make things easier in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df182f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [DOLPHIN_PHI, DEEPSEEK, GLM4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a6014",
   "metadata": {},
   "source": [
    "Now, we can create a _function_ for sending the same prompt to different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b774de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limerick_creator(model: str):\n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Write a limerick about the nature of time.\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(\"Model:\", model)\n",
    "    print(response[\"message\"][\"content\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf4f7a",
   "metadata": {},
   "source": [
    "Now we can _call_ this function with our different models, and see how the output varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28689e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    limerick_creator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053a57f",
   "metadata": {},
   "source": [
    "## Other ML Tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
