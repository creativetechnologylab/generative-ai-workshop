{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef47f24",
   "metadata": {},
   "source": [
    "# Generative AI with Python (with some Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176bcbee",
   "metadata": {},
   "source": [
    "## Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e3641",
   "metadata": {},
   "source": [
    "### Text-to-Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5de9de4",
   "metadata": {},
   "source": [
    "To start with, we need to import the `pytorch` library as well as some useful tools from the `diffusers` library.\n",
    "\n",
    "I asked you to choose an interpreter called `generative-ai-workshop` to run the code. We are using this because the installation scripts you ran earlier installed these specific libraries to a python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff20b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import (\n",
    "    AutoPipelineForText2Image,\n",
    "    DEISMultistepScheduler,\n",
    "    StableDiffusionUpscalePipeline,\n",
    "    DiffusionPipeline,\n",
    "    CogVideoXPipeline,\n",
    "    AutoPipelineForImage2Image,\n",
    ")\n",
    "from PIL import Image\n",
    "from diffusers.utils import export_to_video, make_image_grid\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41556b5",
   "metadata": {},
   "source": [
    "It's good to check if CUDA is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f3bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e2040",
   "metadata": {},
   "source": [
    "The different models will be stored in variables. This is simply for neatness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a369e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "TINY_SD = \"segmind/tiny-sd\"\n",
    "GHIBLI_DIFFUSION = \"nitrosocke/Ghibli-Diffusion\"\n",
    "LCM_DREAMSHAPER = \"SimianLuo/LCM_Dreamshaper_v7\"\n",
    "LYKON_DREAMSHAPER = \"lykon/dreamshaper-8\"\n",
    "KANDINSKY = \"kandinsky-community/kandinsky-2-2-decoder\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee95b4a",
   "metadata": {},
   "source": [
    "We also have to let the code know whether we want to generate content with either Cuda or the CPU. We **always** want to be using Cuda, so let's save the string \"cuda\" as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b86dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd75ed7",
   "metadata": {},
   "source": [
    "Now we can start creating images. We can use the `tiny-sd` model. This model is specifically designed to use very little space, relative to your typical diffusion model. To start with, we can simply ask it to draw \"stonehenge\" and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce1637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the autopipeline method is a one-size-fits-all function for creating images with the diffusers library\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(TINY_SD, torch_dtype=torch.float16)\n",
    "\n",
    "# the prompt determines what the diffusion model will draw\n",
    "prompt = \"stonehenge\"\n",
    "\n",
    "# we are ensuring the image generation happens on our graphics card\n",
    "pipe = pipe.to(CUDA)\n",
    "\n",
    "# now we can create and display the image\n",
    "stonehenge_take_one = pipe(prompt).images[0]\n",
    "display(stonehenge_take_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f458644",
   "metadata": {},
   "source": [
    "Let's try to use a different model to draw for us. This one is called `Ghibli-Diffusion`. We can ask it to draw a castle for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    GHIBLI_DIFFUSION, torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(CUDA)\n",
    "\n",
    "prompt = \"castle\"\n",
    "image = pipe(prompt).images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c00180",
   "metadata": {},
   "source": [
    "But the castle won't look Ghibli-ish unless we provide the trigger phrase. Now we can try again making sure to add \"ghibli style\" to our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"ghibli style, castle\"\n",
    "image = pipe(prompt).images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8085e",
   "metadata": {},
   "source": [
    "Certain models require trigger phrases in order for a certain style to be \"activated.\" This is because they have been built on top of an existing model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3922247",
   "metadata": {},
   "source": [
    "Now let's try to the `dreamshaper-8` model to draw a blue alien guy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285507f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    LYKON_DREAMSHAPER, torch_dtype=torch.float16, variant=\"fp16\"\n",
    ")\n",
    "pipe.scheduler = DEISMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe = pipe.to(CUDA)\n",
    "\n",
    "prompt = \"portrait photo of blue alien man, light bokeh, intricate, elegant, sharp focus, soft lighting, vibrant colors\"\n",
    "\n",
    "image = pipe(prompt).images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0930f2",
   "metadata": {},
   "source": [
    "Now let's take a look at the `LCM_Dreamshaper_v7` model. This one is the most intensive of the bunch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9701e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AutoPipelineForText2Image.from_pretrained(LCM_DREAMSHAPER)\n",
    "pipe.to(torch_device=CUDA, torch_dtype=torch.float16)\n",
    "\n",
    "prompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n",
    "\n",
    "# Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.\n",
    "num_inference_steps = 4\n",
    "image = pipe(\n",
    "    prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0\n",
    ").images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e5d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AutoPipelineForText2Image.from_pretrained(KANDINSKY, dtype=torch.float16)\n",
    "pipe = pipe.to(CUDA)\n",
    "\n",
    "prompt = \"portrait of a young women, blue eyes, cinematic\"\n",
    "\n",
    "image = pipe(prompt=prompt, prior_guidance_scale=1.0, height=768, width=768).images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3166d",
   "metadata": {},
   "source": [
    "#### Prompt Enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90e6bcd",
   "metadata": {},
   "source": [
    "There are certain key phrases that can cause text-to-image models to generate nicer images. Here are some examples of free tools that can help us find these phrases:\n",
    "\n",
    "https://www.neuralframes.com/tools/stable-diffusion-prompt-generator  \n",
    "https://www.feedough.com/stable-diffusion-prompt-generator/\n",
    "\n",
    "Now let's try drawing Stonehenge again, but this time with the other terms suggested by the prompt enhancers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70da9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AutoPipelineForText2Image.from_pretrained(TINY_SD, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(CUDA)\n",
    "\n",
    "enchanced_prompt = \"\"  # COPY AND PASTE HERE\n",
    "\n",
    "# generate an image with the enhanced prompt\n",
    "stonehenge_take_two = pipe(enchanced_prompt).images[0]\n",
    "\n",
    "# display both of our stonehenge images\n",
    "display(stonehenge_take_one, stonehenge_take_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd410c73",
   "metadata": {},
   "source": [
    "#### Negative Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027b413",
   "metadata": {},
   "source": [
    "We can provide **negative prompts** when generating an image. This is a way of telling our model what _not_ to do. For example, let's try using a model to generate an image of a nature-y scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3115116",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    LYKON_DREAMSHAPER, torch_dtype=torch.float16\n",
    ")\n",
    "prompt = \"nature scene, wilderness, hyperrealistic, octane render, 8k, photorealistic, volumetric lighting, epic, dramatic, dark fantasy, National Geographic photo, breathtaking, intricate details, highly detailed, sharp focus, masterpiece.\"\n",
    "pipe = pipe.to(CUDA)\n",
    "\n",
    "image = pipe(prompt).images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700fdac",
   "metadata": {},
   "source": [
    "We can use a negative prompt to make trees and the colour green _less_ prominent in our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e24100",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"green:0.5, grass, trees, scary, animal, beast\"\n",
    "image = pipe(prompt, negative_prompt=negative_prompt).images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b68f1fd",
   "metadata": {},
   "source": [
    "### Upscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c14f1",
   "metadata": {},
   "source": [
    "It is also possible to use models to artificially \"upscale\" an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf45b4b",
   "metadata": {},
   "source": [
    "Let's load a low-resolution image of a cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a086ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res_img = Image.open(\"./low-res-cat.jpg\")\n",
    "display(low_res_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547be957",
   "metadata": {},
   "source": [
    "We can use an upscaler to attempt to make the image larger. To do this, we can use the `stable-diffusion-x4-upscaler` model. This can be \"assisted\" by providing a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-x4-upscaler\"\n",
    "pipeline = StableDiffusionUpscalePipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16\n",
    ")\n",
    "pipeline = pipeline.to(CUDA)\n",
    "\n",
    "prompt = \"A white kitten wearing a jingling bell collar, perched on a plush sapphire sofa, cozy indoor lighting, shallow depth of field, sharp focus, volumetric light, photorealistic, 8k, hyperdetailed, cinematic, editorial photography.\"\n",
    "upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\n",
    "display(upscaled_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85447dab",
   "metadata": {},
   "source": [
    "### Text-to-Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e995e9b",
   "metadata": {},
   "source": [
    "There are also models that can generative videos from text, although the models that can run on your own hardware are going to be a bit more limited. We can ask this model to give us a video of Spiderman skateboarding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"damo-vilab/text-to-video-ms-1.7b\", variant=\"fp16\"\n",
    ")\n",
    "pipe = pipe.to(CUDA)\n",
    "\n",
    "prompt = \"spiderman is skateboarding\"\n",
    "video_frames = pipe(prompt).frames[0]\n",
    "video_path = export_to_video(\n",
    "    video_frames,\n",
    "    output_video_path=\"../output-spiderman.mp4\",\n",
    ")\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d9b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.\"\n",
    "\n",
    "pipe = CogVideoXPipeline.from_pretrained(\n",
    "    \"THUDM/CogVideoX-2b\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "pipe.vae.enable_slicing()\n",
    "pipe.vae.enable_tiling()\n",
    "video = pipe(\n",
    "    prompt=prompt,\n",
    "    num_videos_per_prompt=1,\n",
    "    num_inference_steps=50,\n",
    "    num_frames=49,\n",
    "    guidance_scale=6,\n",
    "    generator=torch.Generator(device=CUDA),\n",
    ").frames[0]\n",
    "\n",
    "video_path = export_to_video(video, \"../output-panda.mp4\", fps=8)\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d008a",
   "metadata": {},
   "source": [
    "### Image to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af09dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_image = Image.open(\"../pictures/clown.jpg\")\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    KANDINSKY, torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "\n",
    "prompt = \"clown in forest\"\n",
    "image = pipeline(prompt, image=init_image).images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d7cca",
   "metadata": {},
   "source": [
    "### AI Weirdness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438f702",
   "metadata": {},
   "source": [
    "We could ask these models to draw _nothing_ and see that happens. They will then start from random noise and..._do whatever._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all of our text-to-image models in a list\n",
    "DIFFUSION_MODELS = [\n",
    "    TINY_SD,\n",
    "    GHIBLI_DIFFUSION,\n",
    "    LCM_DREAMSHAPER,\n",
    "    LYKON_DREAMSHAPER,\n",
    "]\n",
    "\n",
    "# prepare a list for storing our images\n",
    "images = []\n",
    "\n",
    "# loop through the different text-to-image models\n",
    "for model in DIFFUSION_MODELS:\n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(model, torch_dtype=torch.float16)\n",
    "    pipe = pipe.to(CUDA)\n",
    "\n",
    "    # add a picture to our list - use an empty string as a prompt\n",
    "    images.append(pipe(\"\").images[0])\n",
    "\n",
    "# display all of the generated pictures\n",
    "display(*images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8a8b04",
   "metadata": {},
   "source": [
    "### Other Options\n",
    "\n",
    "- ComfyUI\n",
    "- Automatic1111\n",
    "- Foooocus\n",
    "- InvokeAI\n",
    "- SD.Next\n",
    "- VoltaML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91e249b",
   "metadata": {},
   "source": [
    "## Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754f3b4",
   "metadata": {},
   "source": [
    "### What are Large Language Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a17cd",
   "metadata": {},
   "source": [
    "Large Language Models \n",
    "\n",
    "\"auto-correct on steroids\"\n",
    "\n",
    "[A short introduction to LLMs](https://www.youtube.com/watch?v=LPZh9BOjkQs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01640ab8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Ollama & Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf3a8f",
   "metadata": {},
   "source": [
    "**Ollama** is a tool that allows us to run LLMs locally. It can be downloaded and used entirely for _free_.\n",
    "\n",
    "But what does it mean to run something _locally_? That means you're running it _solely_ on your own machine, rather than sending information back and forth with an online service.\n",
    "\n",
    "This has some key advantages:\n",
    "- cost\n",
    "- privacy\n",
    "- doesn't depend on stable/fast internet access\n",
    "- peformance isn't affected by how many other people are using the same online services at a given time\n",
    "\n",
    "To test our Ollama installation, we can see the output from inputting `ollama` in the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b72f1",
   "metadata": {},
   "source": [
    "It's also possible to do this within Python by using the `subprocess` library. So that's one option..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run the `echo` command and capture output\n",
    "result = subprocess.run([\"ollama\"], text=True)\n",
    "\n",
    "print(\"Output from command line:\")\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f3473",
   "metadata": {},
   "source": [
    "This gives us a list of commands that we can use with Ollama. For our purposes, we're mainly concerned with being able to pull models, list what models are on our system, and remove the ones we no longer want to use. In a fresh installtion, Ollama comes with zero models, but the script has _pulled_ a few already to make things easier. We can use the ollama-python library to list these models. Of course, the command line works too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d1ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.list().models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676a80e",
   "metadata": {},
   "source": [
    "That is quite a bit of information, so we can go through this _return value_ to extract just the information that's more human-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ollama.list().models:\n",
    "    print(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202453ea",
   "metadata": {},
   "source": [
    "Now we have a plain list of the models on the system - this shows us what was downloaded (or _pulled_) by running the installation script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2dbfc",
   "metadata": {},
   "source": [
    "To start with, I'm going to create a _variable_ for storing the name of the model I wish to use. This is going to be a _parameter_ that we give repeatedly to the ollama python library, so it makes sense to write it down once and avoid repeating ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd9a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dophin-phi is 2.7b\n",
    "DOLPHIN_PHI = \"dolphin-phi\"\n",
    "# this particular deepseek model is 7b\n",
    "DEEPSEEK = \"deepseek-r1:7b\"\n",
    "# glm4 9b version\n",
    "GLM4 = \"glm4:latest\"\n",
    "# moondream\n",
    "MOONDREAM = \"moondream\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a1111",
   "metadata": {},
   "source": [
    "A convention when programming in Python is to write constants -- variables that are set once and never changes -- in all-caps. This doesn't affect how your code runs, but it can be nice for making things more ordered. I feel it tells me this bit of information is \"important\" in some way, while using less mental effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "response = chat(\n",
    "    model=DOLPHIN_PHI,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87600bd5",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = chat(\n",
    "    model=DOLPHIN_PHI,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8907377",
   "metadata": {},
   "source": [
    "### Vision Language Models (VLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564dff12",
   "metadata": {},
   "source": [
    "Vision Language Models can be used to describe images. Let's try this out with this clown image.\n",
    "\n",
    "![](../pictures/clown.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de818b20",
   "metadata": {},
   "source": [
    "First, we need to load the image. To do this, we need ot make use of the `base64` library as it allows us to convert the image into a format that a VLM can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da96d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# load an image as base64\n",
    "with open(\"../pictures/clown.jpg\", \"rb\") as image_file:\n",
    "    data = base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd6efa",
   "metadata": {},
   "source": [
    "Now that the image has been loaded, we can send it to the VLM `moondream`, and ask it to tell us what the image contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33443f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "    model=MOONDREAM,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's in this image?\",\n",
    "            \"images\": [data],  # pass the image in the images field\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35931289",
   "metadata": {},
   "source": [
    "We can also ask moondream to explain certain details in the image to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fea084",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "    model=MOONDREAM,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What colour is his hair?\",\n",
    "            \"images\": [data],  # pass the image in the images field\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae59208",
   "metadata": {},
   "source": [
    "We can see a list of vision models that work with Ollama here: https://ollama.com/search?c=vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3468c26c",
   "metadata": {},
   "source": [
    "### Small Language Models\n",
    "\n",
    "Language Models come in very small sizes too. Some examples include `smollm` and `tinyllama`. While these models are more prone to hallucination, and have more limited \"intelligence,\" they can run quite fast even on less powerful hardware such as Raspberry Pis and computers with older GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e60b6b",
   "metadata": {},
   "source": [
    "### Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91aee88",
   "metadata": {},
   "source": [
    "![](../pictures/how-to-cook-your-dragon.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d135c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\n",
    "    model=DOLPHIN_PHI,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are some good cookbooks on how to use dragon meat?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34632e5",
   "metadata": {},
   "source": [
    "### Thinking Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda3a7b",
   "metadata": {},
   "source": [
    "Explanation of thinking/reasoning models goes here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f879955",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\n",
    "    model=DEEPSEEK,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893821b",
   "metadata": {},
   "source": [
    "#### The Strawberry Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4788485",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = chat(\n",
    "    model=DEEPSEEK,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many times does the letter R appear in the word strawberry?\",\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b51356",
   "metadata": {},
   "source": [
    "### Finding the \"Best\" Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c442ec9",
   "metadata": {},
   "source": [
    "trade-offs with sensible output and size/speed  \n",
    "trial and error experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6a628",
   "metadata": {},
   "source": [
    "We can create a quick comparison test by asking various models to generate text based on the same prompt, and see which output we like the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc8c92d",
   "metadata": {},
   "source": [
    "Firstly, we can take all the models that are on the system right now, and place them in a Python list. This will make things easier in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df182f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [DOLPHIN_PHI, DEEPSEEK, GLM4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a6014",
   "metadata": {},
   "source": [
    "Now, we can create a _function_ for sending the same prompt to different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b774de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limerick_creator(model: str):\n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Write a limerick about the nature of time.\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(\"Model:\", model)\n",
    "    print(response[\"message\"][\"content\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf4f7a",
   "metadata": {},
   "source": [
    "Now we can _call_ this function with our different models, and see how the output varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28689e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    limerick_creator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053a57f",
   "metadata": {},
   "source": [
    "## Other ML Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d6db06",
   "metadata": {},
   "source": [
    "### Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b30c1",
   "metadata": {},
   "source": [
    "The whisper library is capable of understanding audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f144ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import IPython\n",
    "\n",
    "model = whisper.load_model(\"turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc988923",
   "metadata": {},
   "source": [
    "This function will tell us the detected language of some audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_audio_and_detect_language(audio):\n",
    "\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "    # make log-Mel spectrogram and move to the same device as the model\n",
    "    mel = whisper.log_mel_spectrogram(audio, n_mels=128).to(model.device)\n",
    "\n",
    "    # detect the spoken language\n",
    "    _, probs = model.detect_language(mel)\n",
    "    detected_language = max(probs, key=probs.get)\n",
    "\n",
    "    # decode the audio\n",
    "    options = whisper.DecodingOptions()\n",
    "    result = whisper.decode(model, mel, options)\n",
    "\n",
    "    return detected_language, result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dabf0c8",
   "metadata": {},
   "source": [
    "Now let's have a look at an file with some introductory German speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f78dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "GERMAN_AUDIO_PATH = \"../audio-files/german.wav\"\n",
    "IPython.display.Audio(GERMAN_AUDIO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9a6bf",
   "metadata": {},
   "source": [
    "Now we can use whisper to determine the language and tell us what's being said."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = whisper.load_audio(GERMAN_AUDIO_PATH)\n",
    "lang, content = prepare_audio_and_detect_language(audio)\n",
    "\n",
    "print(lang)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47750c59",
   "metadata": {},
   "source": [
    "### OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open(\"../object-detection/yolov3.txt\", \"r\") as f:\n",
    "    CLASSES = [line.strip() for line in f.readlines()]\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "\n",
    "\n",
    "def get_output_layers(net):\n",
    "\n",
    "    layer_names = net.getLayerNames()\n",
    "    try:\n",
    "        output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    except:\n",
    "        output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "\n",
    "def draw_prediction(img, class_id, x, y, x_plus_w, y_plus_h):\n",
    "\n",
    "    label = str(CLASSES[class_id])\n",
    "\n",
    "    color = COLORS[class_id]\n",
    "\n",
    "    cv2.rectangle(img, (x, y), (x_plus_w, y_plus_h), color, 2)\n",
    "\n",
    "    cv2.putText(img, label, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e04273",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"../object-detection/person.jpg\")\n",
    "display(Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d0999",
   "metadata": {},
   "outputs": [],
   "source": [
    "Width = image.shape[1]\n",
    "Height = image.shape[0]\n",
    "scale = 0.00392\n",
    "\n",
    "net = cv2.dnn.readNet(\n",
    "    \"../object-detection/yolov3.weights\", \"../object-detection/yolov3.cfg\"\n",
    ")\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(image, scale, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "net.setInput(blob)\n",
    "\n",
    "outs = net.forward(get_output_layers(net))\n",
    "\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "conf_threshold = 0.5\n",
    "nms_threshold = 0.4\n",
    "\n",
    "\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            center_x = int(detection[0] * Width)\n",
    "            center_y = int(detection[1] * Height)\n",
    "            w = int(detection[2] * Width)\n",
    "            h = int(detection[3] * Height)\n",
    "            x = center_x - w / 2\n",
    "            y = center_y - h / 2\n",
    "            class_ids.append(class_id)\n",
    "            confidences.append(float(confidence))\n",
    "            boxes.append([x, y, w, h])\n",
    "\n",
    "\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "\n",
    "for i in indices:\n",
    "    try:\n",
    "        box = boxes[i]\n",
    "    except:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "\n",
    "    x = box[0]\n",
    "    y = box[1]\n",
    "    w = box[2]\n",
    "    h = box[3]\n",
    "    draw_prediction(image, class_ids[i], round(x), round(y), round(x + w), round(y + h))\n",
    "\n",
    "display(Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a585e",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "### StableDiffusion\n",
    "\n",
    "Negative Prompts: https://blog.segmind.com/beginners-guide-to-understanding-negative-prompts-in-stable-diffusion/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
