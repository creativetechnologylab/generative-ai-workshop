{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef47f24",
   "metadata": {},
   "source": [
    "# Generative AI with Python (with some Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e271a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176bcbee",
   "metadata": {},
   "source": [
    "## Text to Image with StableDiffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91e249b",
   "metadata": {},
   "source": [
    "## Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754f3b4",
   "metadata": {},
   "source": [
    "### What are Large Language Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a17cd",
   "metadata": {},
   "source": [
    "Large Language Models \n",
    "\n",
    "\"auto-correct on steroids\"\n",
    "\n",
    "[A short introduction to LLMs](https://www.youtube.com/watch?v=LPZh9BOjkQs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01640ab8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Ollama & Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf3a8f",
   "metadata": {},
   "source": [
    "**Ollama** is a tool that allows us to run LLMs locally. It can be downloaded and used entirely for _free_.\n",
    "\n",
    "But what does it mean to run something _locally_? That means you're running it _solely_ on your own machine, rather than sending information back and forth with an online service.\n",
    "\n",
    "This has some key advantages:\n",
    "- cost\n",
    "- privacy\n",
    "- doesn't depend on stable/fast internet access\n",
    "- peformance isn't affected by how many other people are using the same online services at a given time\n",
    "\n",
    "To test our Ollama installation, we can see the output from inputting `ollama` in the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b72f1",
   "metadata": {},
   "source": [
    "It's also possible to do this within Python by using the `subprocess` library. So that's one option..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0af523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from command line:\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage:\n",
      "  ollama [flags]\n",
      "  ollama [command]\n",
      "\n",
      "Available Commands:\n",
      "  serve       Start ollama\n",
      "  create      Create a model from a Modelfile\n",
      "  show        Show information for a model\n",
      "  run         Run a model\n",
      "  stop        Stop a running model\n",
      "  pull        Pull a model from a registry\n",
      "  push        Push a model to a registry\n",
      "  list        List models\n",
      "  ps          List running models\n",
      "  cp          Copy a model\n",
      "  rm          Remove a model\n",
      "  help        Help about any command\n",
      "\n",
      "Flags:\n",
      "  -h, --help      help for ollama\n",
      "  -v, --version   Show version information\n",
      "\n",
      "Use \"ollama [command] --help\" for more information about a command.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run the `echo` command and capture output\n",
    "result = subprocess.run([\"ollama\"], text=True)\n",
    "\n",
    "print(\"Output from command line:\")\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f3473",
   "metadata": {},
   "source": [
    "This gives us a list of commands that we can use with Ollama. For our purposes, we're mainly concerned with being able to pull models, list what models are on our system, and remove the ones we no longer want to use. In a fresh installtion, Ollama comes with zero models, but the script has _pulled_ a few already to make things easier. We can use the ollama-python library to list these models. Of course, the command line works too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831d1ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(model='glm4:latest', modified_at=datetime.datetime(2025, 7, 16, 15, 7, 31, 79556, tzinfo=TzInfo(+01:00)), digest='5b699761eca535dc55047ad9d2dbf54e3b8697709419ef78a70503ed4bfbcf44', size=5455326235, details=ModelDetails(parent_model='', format='gguf', family='chatglm', families=['chatglm'], parameter_size='9.4B', quantization_level='Q4_0')),\n",
       " Model(model='deepseek-r1:7b', modified_at=datetime.datetime(2025, 7, 16, 14, 52, 22, 708686, tzinfo=TzInfo(+01:00)), digest='755ced02ce7befdb13b7ca74e1e4d08cddba4986afdb63a480f2c93d3140383f', size=4683075440, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='7.6B', quantization_level='Q4_K_M')),\n",
       " Model(model='dolphin-phi:latest', modified_at=datetime.datetime(2025, 7, 16, 11, 21, 6, 952843, tzinfo=TzInfo(+01:00)), digest='c5761fc772409945787240af89a5cce01dd39dc52f1b7b80d080a1163e8dbe10', size=1602473850, details=ModelDetails(parent_model='', format='gguf', family='phi2', families=['phi2'], parameter_size='3B', quantization_level='Q4_0')),\n",
       " Model(model='cogito:32b', modified_at=datetime.datetime(2025, 5, 5, 5, 19, 4, 7661, tzinfo=TzInfo(+01:00)), digest='0b4aab772f57ddf3d9e46235ef6f142e42598fd121dc15588c7f06de786c510e', size=19848516836, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='32.8B', quantization_level='Q4_K_M')),\n",
       " Model(model='JollyLlama/GLM-4-32B-0414-Q4_K_M:latest', modified_at=datetime.datetime(2025, 5, 2, 0, 25, 50, 275690, tzinfo=TzInfo(+01:00)), digest='d61b44b6a5d3234dd25dd525546e895b9431a3b2b798666d154844fdeb3e0129', size=19680022745, details=ModelDetails(parent_model='', format='gguf', family='glm4', families=['glm4'], parameter_size='32.6B', quantization_level='Q4_K_M')),\n",
       " Model(model='EntropyYue/longwriter-glm4:9b', modified_at=datetime.datetime(2025, 5, 1, 23, 44, 15, 434703, tzinfo=TzInfo(+01:00)), digest='6fb2068f2b4362b0b5e6d54c907007e854d5a7347bd8ebe3324defbb027b8fe8', size=5455324148, details=ModelDetails(parent_model='', format='gguf', family='chatglm', families=['chatglm'], parameter_size='9.4B', quantization_level='Q4_0'))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.list().models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676a80e",
   "metadata": {},
   "source": [
    "That is quite a bit of information, so we can go through this _return value_ to extract just the information that's more human-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "af2f2ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm4:latest\n",
      "deepseek-r1:7b\n",
      "dolphin-phi:latest\n",
      "cogito:32b\n",
      "JollyLlama/GLM-4-32B-0414-Q4_K_M:latest\n",
      "EntropyYue/longwriter-glm4:9b\n"
     ]
    }
   ],
   "source": [
    "for model in ollama.list().models:\n",
    "    print(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202453ea",
   "metadata": {},
   "source": [
    "Now we have a plain list of the models on the system - this shows us what was downloaded (or _pulled_) by running the installation script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2dbfc",
   "metadata": {},
   "source": [
    "To start with, I'm going to create a _variable_ for storing the name of the model I wish to use. This is going to be a _parameter_ that we give repeatedly to the ollama python library, so it makes sense to write it down once and avoid repeating ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd9a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dophin-phi is 2.7b\n",
    "DOLPHIN_PHI = \"dolphin-phi\"\n",
    "# this particular deepseek model is 7b\n",
    "DEEPSEEK = \"deepseek-r1:7b\"\n",
    "# glm4 9b version\n",
    "GLM4 = \"glm4:latest\"\n",
    "# moondream\n",
    "MOONDREAM = \"moondream\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a1111",
   "metadata": {},
   "source": [
    "A convention when programming in Python is to write constants -- variables that are set once and never changes -- in all-caps. This doesn't affect how your code runs, but it can be nice for making things more ordered. I feel it tells me this bit of information is \"important\" in some way, while using less mental effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "97bf6360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris. It is also one of the most populous cities in Europe and well-known for its rich history, art, fashion, and culture. The city is located on the banks of the River Seine. Some popular attractions include the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and many more.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "response = chat(model=DOLPHIN_PHI, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'What is the capital of France?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87600bd5",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89a0c623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue due to the phenomenon known as Rayleigh scattering. When sunlight enters the Earth's atmosphere, it interacts with molecules in the air such as nitrogen and oxygen. These molecules are much smaller than the wavelength of visible light (around 400-700 nm), which is why they can scatter the shorter-wavelength colors more effectively, like blue.\n",
      "\n",
      "However, the sky isn't really blue at all. The light from the sun gets scattered in all directions by these molecules in the atmosphere and our eyes perceive this as a \"blue\" color. When the sunlight passes through Earth's atmosphere, it is broken up into its different colors due to Rayleigh scattering. Blue light has a shorter wavelength, so it is scattered more than the other colors of visible light, which are longer in wavelength. This makes the sky appear blue from our perspective on the ground.\n",
      "\n",
      "The deeper you look into the sky, the further away the sun is and the longer the path sunlight travels, causing more scattering of shorter wavelengths of light to be absorbed by the atmosphere. At sunset or sunrise when the sun is low on the horizon, the light has to pass through a larger portion of Earth's atmosphere, which scatters even more blue light and causes the sky to turn various shades of red and orange."
     ]
    }
   ],
   "source": [
    "stream = chat(\n",
    "    model=DOLPHIN_PHI,\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8907377",
   "metadata": {},
   "source": [
    "### Vision Language Models (VLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564dff12",
   "metadata": {},
   "source": [
    "Vision Language Models can be used to describe images. Let's try this out with this clown image.\n",
    "\n",
    "![](../pictures/clown.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de818b20",
   "metadata": {},
   "source": [
    "First, we need to load the image. To do this, we need ot make use of the `base64` library as it allows us to convert the image into a format that a VLM can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da96d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# load an image as base64\n",
    "with open(\"../pictures/clown.jpg\", \"rb\") as image_file:\n",
    "    data = base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd6efa",
   "metadata": {},
   "source": [
    "Now that the image has been loaded, we can send it to the VLM `moondream`, and ask it to tell us what the image contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c33443f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The image features a clown with a red wig and polka dot suit, standing against a white background. The clown is making a funny face for the camera while holding up his hands as if to mimic a speech bubble or an exaggerated \"O\" shape.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=MOONDREAM,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's in this image?\",\n",
    "            \"images\": [data], # pass the image in the images field\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35931289",
   "metadata": {},
   "source": [
    "We can also ask moondream to explain certain details in the image to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81fea084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The clown's nose is red.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=MOONDREAM,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What colour is his nose?\",\n",
    "            \"images\": [data], # pass the image in the images field\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae59208",
   "metadata": {},
   "source": [
    "We can see a list of vision models that work with Ollama here: https://ollama.com/search?c=vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3468c26c",
   "metadata": {},
   "source": [
    "### Small Language Models\n",
    "\n",
    "Language Models come in very small sizes too. Some examples include `smollm` and `tinyllama`. While these models are more prone to hallucination, and have more limited \"intelligence,\" they can run quite fast even on less powerful hardware such as Raspberry Pis and computers with older GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e60b6b",
   "metadata": {},
   "source": [
    "### Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91aee88",
   "metadata": {},
   "source": [
    "![](../pictures/how-to-cook-your-dragon.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "07d135c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"Dragon Meat Cookbook\" by John Doe - This comprehensive cookbook offers various recipes using dragon meat, along with detailed cooking techniques and nutritional information.\n",
      "2. \"Cooking with Dragon Meat: A Guide for the Adventurous Chef\" by Jane Smith - It provides a wide range of creative and delicious recipes that showcase the versatility of dragon meat.\n",
      "3. \"Dragon Cuisine: The Ultimate Guide to Cooking with Dragon Meat\" by Michael Johnson - This book not only offers tasty recipes but also delves into the cultural and historical aspects of cooking with dragon meat.\n",
      "4. \"The Dragon's Feast Cookbook\" by Sarah Lee - A collection of authentic recipes from different regions that feature dragon meat, making it a great resource for global culinary exploration.\n",
      "5. \"Dragon Meat: Cooking with a Mythical Ingredient\" by David Brown - This book focuses on the unique flavors and textures of dragon meat, providing guidance on how to cook it in a variety of dishes.\n",
      "6. \"The Dragon Chef's Cookbook\" by Emily Chen - A collection of easy-to-follow recipes that cater to different dietary preferences, making it an accessible resource for home cooks.\n",
      "7. \"Dragon Meat: The Culinary Adventure\" by William Taylor - This cookbook is packed with creative and innovative dragon meat recipes, along with tips on how to source and prepare the meat.\n",
      "8. \"The Dragon Cookbook\" by Jennifer Lee - A unique and entertaining guide that features a collection of funny anecdotes, trivia, and stories related to dragon meat.\n",
      "9. \"Dragon Meats: From Dragon Roasting to Dragon Scallop\" by Robert Brown - This in-depth guide provides expert insights into the history, preparation, and culinary uses of dragon meat.\n",
      "10. \"The Dragon Chef's Guide to Cooking with Fire\" by Samantha Smith - A resource for those looking to cook with a variety of meats, including dragon, over an open fire.\n"
     ]
    }
   ],
   "source": [
    "response = chat(model=DOLPHIN_PHI, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'What are some good cookbooks on how to use dragon meat?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34632e5",
   "metadata": {},
   "source": [
    "### Thinking Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b51356",
   "metadata": {},
   "source": [
    "### Finding the \"Best\" Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c442ec9",
   "metadata": {},
   "source": [
    "trade-offs with sensible output and size/speed  \n",
    "trial and error experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6a628",
   "metadata": {},
   "source": [
    "We can create a quick comparison test by asking various models to generate text based on the same prompt, and see which output we like the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc8c92d",
   "metadata": {},
   "source": [
    "Firstly, we can take all the models that are on the system right now, and place them in a Python list. This will make things easier in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df182f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [DOLPHIN_PHI, DEEPSEEK, GLM4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a6014",
   "metadata": {},
   "source": [
    "Now, we can create a _function_ for sending the same prompt to different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b774de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limerick_creator(model: str):\n",
    "    response = chat(model=model, messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Write a limerick about the nature of time.',\n",
    "    },\n",
    "    ])\n",
    "    \n",
    "    print(\"Model:\", model)\n",
    "    print(response['message']['content'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf4f7a",
   "metadata": {},
   "source": [
    "Now we can _call_ this function with our different models, and see how the output varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28689e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: dolphin-phi\n",
      "There once was a concept called time,\n",
      "Its ticking clock could not be chime,\n",
      "It flowed like a river swift,\n",
      "Past and future with a shift,\n",
      "Leaving moments in its wake so bright.\n",
      "\n",
      "\n",
      "Model: deepseek-r1:7b\n",
      "<think>\n",
      "Alright, so I need to write a limerick about the nature of time. Hmm, okay. First off, what is a limerick? From what I remember, it's a five-line poem with an AABBA rhyme scheme. It usually has a playful or rhythmic feel to it and often tells a short story or conveys a light-hearted message.\n",
      "\n",
      "Now, the topic here is the nature of time. Time can be tricky because it's something we all experience every day but isn't tangible—it's more of an abstract concept. I guess I could approach this in different ways: maybe talking about how time moves us forward without pause, or perhaps the idea that time doesn't care who waits for it.\n",
      "\n",
      "I should think about imagery related to time—maybe clocks, the ticking sound, seasons passing, aging, or moments rushing by too quickly. Using some rhyme and rhythm here will be key. Let me try to come up with a structure: first line introduces the subject of time; second and third lines delve into its nature or behavior; fourth line might offer a perspective on how it's perceived; fifth line is usually a punchline or a conclusion.\n",
      "\n",
      "Let me see... Maybe start with something like \"There once was a time, oh what fun,\" but wait, that doesn't fit the limerick structure. Alternatively, I could try to think of a metaphor involving movement since time often feels like it's moving forward.\n",
      "\n",
      "Wait, another thought: perhaps using the image of a river running or a child skipping stones over water because time glides by quickly. That could tie into the idea of time passing without stopping and how we don't notice it when it does go by.\n",
      "\n",
      "Putting that together:\n",
      "\n",
      "\"There once was a river so clear,\"  \n",
      "\"Where time's waves seemed to skip.\"  \n",
      "But then I need to fit in something about its nature, maybe how time doesn't wait for anyone. Let me try another angle: comparing time to something that's always moving without rest.\n",
      "\n",
      "Maybe something like \"Time has no bounds, it goes on and on,\" but make it rhyme. Hmm, perhaps using \"snaps through the fingers of man\" from Alice in Wonderland as a metaphor? Not sure if that's the best fit here.\n",
      "\n",
      "Wait, let me try to think of some lines:\n",
      "\n",
      "\"The clock ticks softly, soft as a bell,\"  \n",
      "\"But time is faster, quicker than we'll see.\"  \n",
      "But then I need more lines. Maybe add something about how it doesn't stop for anyone and how we often resist its flow.\n",
      "\n",
      "Wait, maybe the first line could be \"Time is a thief\" or \"Time moves swiftly.\" Let me try that:\n",
      "\n",
      "\"There's a thief in the night, quick as can be,\"  \n",
      "\"Who steals away moments when we're not asleep.\"  \n",
      "That has rhyme and rhythm. Now for the next lines: perhaps how time doesn't wait for anyone.\n",
      "\n",
      "\"But he takes what he wants, with little pause,\"  \n",
      "\"He leaves our days before our years can save.\"  \n",
      "Now I'm at four lines. The fifth line should wrap it up or add a twist. Maybe something like \"So hurry up and be ready\" to catch up? But that's more of a command than a statement about time.\n",
      "\n",
      "Alternatively, maybe the last couplet could express a perspective on life in relation to time:\n",
      "\n",
      "\"So watch the moments as they pass by,\"  \n",
      "\"Time's a trickster, always changes my way.\"  \n",
      "Hmm, not sure if that's strong enough. Let me try again with different wording for the last two lines.\n",
      "\n",
      "Perhaps something like \"Time moves on without pause, It takes what it will, and leaves its trace.\"\n",
      "\n",
      "Wait, putting it all together:\n",
      "\n",
      "\"There was once a thief in the night,\"  \n",
      "\"Who steals away moments when we rest.\"  \n",
      "\"He takes what he wants, with little delay,\"  \n",
      "\"And leaves our days before our years grow old.\"  \n",
      "\"So watch the moments as they pass by,\"  \n",
      "\"Time's a trickster, always changes my way.\"\n",
      "\n",
      "Hmm, that works. It follows the AABBA rhyme scheme and touches on time moving quickly without pause, taking what it wants, leaving in its wake, making us realize how fast our days fly.\n",
      "\n",
      "I think this captures the essence of the limerick structure while conveying something about time's nature—its swift movement and impact on our lives.\n",
      "</think>\n",
      "\n",
      "There once was a thief in the night,  \n",
      "Who steals away moments when we rest.  \n",
      "He takes what he wants, with little delay,  \n",
      "And leaves our days before our years grow old.  \n",
      "\n",
      "So watch the moments as they pass by,  \n",
      "Time's a trickster, always changes my way.\n",
      "\n",
      "\n",
      "Model: glm4:latest\n",
      "There once was time so swift and fleet,  \n",
      "With moments that could seem complete;  \n",
      "But then it's past, as if a dream,  \n",
      "And each second dances in a beam —  \n",
      "Nature’s clock keeps on a tick.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    limerick_creator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053a57f",
   "metadata": {},
   "source": [
    "## Other ML Tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
